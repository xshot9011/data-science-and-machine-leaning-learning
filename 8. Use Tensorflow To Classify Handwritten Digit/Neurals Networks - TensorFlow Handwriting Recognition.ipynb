{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\b\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\b\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\b\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\b\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\b\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\b\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\b\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\b\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\b\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\b\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\b\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\b\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(888)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(404)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from time import strftime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN_PATH = 'data/digit_xtrain.csv'\n",
    "X_TEST_PATH = 'data/digit_xtest.csv'\n",
    "Y_TRAIN_PATH = 'data/digit_ytrain.csv'\n",
    "Y_TEST_PATH = 'data/digit_ytest.csv'\n",
    "\n",
    "LOGGING_PATH = 'tensorboard_mnist_digit_logs/'\n",
    "\n",
    "NUMBER_CLASSES = 10\n",
    "VALIDATION_SIZE = 10000\n",
    "\n",
    "IMAGE_WIDTH = 28\n",
    "IMAGE_HEIGHT = 28\n",
    "CHANNELS = 1\n",
    "TOTAL_INPUTS = IMAGE_HEIGHT * IMAGE_WIDTH * CHANNELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "y_train_all = np.loadtxt(Y_TRAIN_PATH, delimiter=',', dtype=int)\n",
    "x_train_all = np.loadtxt(X_TRAIN_PATH, delimiter=',', dtype=int)\n",
    "x_test = np.loadtxt(X_TEST_PATH, delimiter=',', dtype=int)\n",
    "y_test = np.loadtxt(Y_TEST_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore & Visualise Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "รูปภาพเราในที่นี้มีคนจัดการทำให้มันเป็น array มาให้แล้ว ไม่ต้องทำการคอนเวิทเอง มีขนาด 28 * 28 และมี 1 channel\n",
    "28 * 28 * 1 = 784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape : (60000, 784)\n",
      "x_test shape : (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "print('x_train shape :', x_train_all.shape)\n",
    "# print(x_train_all[0])\n",
    "print('x_test shape :', x_test.shape)\n",
    "# all shape have been flatten, if not >> (number of datapoint, w, h, channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-scale to 0-1\n",
    "x_train_all, x_test = x_train_all/255.0, x_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = y_train_all[:5]\n",
    "np.eye(10)[values]  # given values is consider as position in each row to represent 1 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conver target values to one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "(10000, 10)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "y_train_all = np.eye(NUMBER_CLASSES)[y_train_all]\n",
    "print(y_train_all.shape)\n",
    "y_test = np.eye(NUMBER_CLASSES)[y_test]\n",
    "print(y_test.shape)\n",
    "\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_validation = x_train_all[:VALIDATION_SIZE]\n",
    "y_validation = y_train_all[:VALIDATION_SIZE]\n",
    "x_train = x_train_all[VALIDATION_SIZE:]\n",
    "y_train = y_train_all[VALIDATION_SIZE:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Tensorflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rerun all below\n",
    "X = tf.placeholder(tf.float32, shape=[None, TOTAL_INPUTS], name='X')  \n",
    "# we leave the first dimension black, first dim is how many example we are going to use\n",
    "# this will actually be determined a little later on, when training model, we will split data to batch\n",
    "Y = tf.placeholder(tf.float32, shape=[None, NUMBER_CLASSES], name='labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "\n",
    "### Hyperparameters\n",
    "hyperparameter is something that don't come out of training the model.\n",
    "A parameter that comes out of training the model would be something like the weights.\n",
    "So these are things like how long to train our model for, number of epoch used for training, the learning rate use in out optimizer, the number of nodes per layer\n",
    "\n",
    "all above is hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_number = 50\n",
    "learning_rate = 0.0001  # 1e-4\n",
    "\n",
    "hidden_1_number = 512\n",
    "hidden_2_number = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will give some starting values to our weight and bias for neural network.\n",
    "all the connection weights in the neurals networks need some sort of initail value.\n",
    "we will give a small random values at the strating point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w_1 = tf.truncated_normal(shape=[TOTAL_INPUTS, hidden_1_number], stddev=0.1, seed=42)  # the number of input, number of nueron\n",
    "w1 = tf.Variable(initial_value=initial_w_1)  # create tensorflow's variable to hold all the weight in the first hidden layers\n",
    "# weight has to be persist and updated as all the calcautions intense floor are going to be run, this is why we carete them as Variabel\n",
    "initial_bias_1 = tf.constant(value=0.0, shape=[hidden_1_number])# bias is shift of the avtivation function for all node in that layer\n",
    "b1 = tf.Variable(initial_value=initial_bias_1)\n",
    "# bias and weight for this first hidden layer will be updated during the training process.\n",
    "# This is where the learning occur\n",
    "# These are value, fed into the activation functions of the nuerons and represent the strength of the connection between the units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow has this like 2 stage approachs. <br>\n",
    "The first stage is all set up <br>\n",
    "The second stage that these calculation are actually all done <br>\n",
    "So as long as we don't actually tell tensorflow to evaluate any of these tensor and to run all the calculation, we don't actually get to see the value that are generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'truncated_normal:0' shape=(784, 512) dtype=float32>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_w_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need to determine how the weight and the bias work together to determine the input in the hidden layer\n",
    "\n",
    "[w_i] * [o_i] = [in_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1_input = tf.matmul(X, w1) + b1  # to multiply 2 matrix \n",
    "layer_1_output = tf.nn.relu(layer_1_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** challenge ** set up all layer later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w_2 = tf.truncated_normal(shape=[hidden_1_number, hidden_2_number], stddev=0.1, seed=42)\n",
    "w_2 = tf.Variable(initial_value=initial_w_2)\n",
    "initial_bias_2 = tf.constant(value=0.0, shape=[hidden_2_number])\n",
    "b_2 = tf.Variable(initial_value=initial_bias_2)\n",
    "\n",
    "layer_2_input = tf.matmul(layer_1_output, w_2) + b_2\n",
    "layer_2_output = tf.nn.relu(layer_2_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w_output = tf.truncated_normal(shape=[hidden_2_number, NUMBER_CLASSES], stddev=0.1, seed=42)\n",
    "w_output = tf.Variable(initial_value=initial_w_output)\n",
    "initial_bias_output = tf.constant(value=0.0, shape=[NUMBER_CLASSES])\n",
    "b_output = tf.Variable(initial_value=initial_bias_output)\n",
    "\n",
    "o_input = tf.matmul(layer_2_output, w_output) + b_output\n",
    "output = tf.nn.softmax(o_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully create file\n"
     ]
    }
   ],
   "source": [
    "name = 'model'\n",
    "folder_name = f'Model {name} at {strftime(\"%D %H%M\")}'.replace('/', ';')\n",
    "directory = os.path.join(LOGGING_PATH, folder_name)\n",
    "\n",
    "try:\n",
    "    os.makedirs(directory)\n",
    "except OSError as oserr:\n",
    "    print('error :', oserr.strerror)\n",
    "else:\n",
    "    print('Successfully create file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss, Optimisation and Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create loss function >> google >> tensorflow loss funtion\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=output))\n",
    "# this loss calculation work for calculating the loss on the entire dataset\n",
    "# it will not give a good result when we've got individual batches\n",
    "# so when we have individual batches is we need to take the average of the losses\n",
    "# by using tf.reduce_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Optimisation >> google >> tensorflow Optimisation\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_step = optimizer.minimize(loss)\n",
    "# all above is very quite complex, this is why we have keras lol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to create accuracy we need to compare 2 things, the predictions and true labels\n",
    "correct_pred = tf.equal(tf.argmax(output, axis=1), tf.argmax(Y, axis=1))\n",
    "# get the maximun from both prediction values and labels\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'loss:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.summary.scalar('accuracy', accuracy)\n",
    "tf.summary.scalar('loss', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run session\n",
    "\n",
    "Tensorflow has session object. <br>\n",
    "Session objects encapsulate the environment under which all the operation and all the calculations take place and are executed. <br>\n",
    "All of setting, calculation take place inisde a session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "# initialise all the variables\n",
    "init = tf.global_variables_initializer()\n",
    "session.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Filewriter and Merge Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_summary = tf.summary.merge_all()  # all the calcalation whta we want to read, we will combine and merge them and store it to this var\n",
    "train_writer = tf.summary.FileWriter(directory + '/train')\n",
    "train_writer.add_graph(session.graph)\n",
    "# all of the calculation are in thing called 'graph' >> graph\n",
    "# a calculation are evaluate during the session >> session.graph \n",
    "# for a file writer to know which calculation to save and write down to the disk\n",
    "# we have to tell it which graph to use [].add_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_writer = tf.summary.FileWriter(directory + '/validation')\n",
    "# in the theory i should add_graph to the validation_writer as well\n",
    "# but validation_writer and train_writer actually using the smae graph\n",
    "# validation_writer.add_graph(session.grapg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### to See the values inside the var that we created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now if you want to see the value inside tensor. >> evaluate >> eval(session)\n",
    "b_2.eval(session)\n",
    "# it's like a pipe when we actually run the session the data start flowing through these pipe\n",
    "# then we can evaluate the calculation and get output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_of_batch = 1000\n",
    "examples_number = y_train.shape[0]\n",
    "iteration_number = int(examples_number/size_of_batch)\n",
    "\n",
    "epoch_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(size_of_batch, data, labels):\n",
    "    global examples_number\n",
    "    global epoch_index\n",
    "    \n",
    "    start = epoch_index\n",
    "    epoch_index += size_of_batch\n",
    "    if epoch_index > examples_number:\n",
    "        start = 0\n",
    "        epoch_index = size_of_batch\n",
    "    end = epoch_index\n",
    "    \n",
    "    return data[start:end], labels[start:end]  # 0:1000(999), 1000:2000(1999), ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 \t| Training Accuracy : 0.6380000114440918\n",
      "Epoch : 1 \t| Training Accuracy : 0.8040000200271606\n",
      "Epoch : 2 \t| Training Accuracy : 0.8450000286102295\n",
      "Epoch : 3 \t| Training Accuracy : 0.8600000143051147\n",
      "Epoch : 4 \t| Training Accuracy : 0.871999979019165\n",
      "Epoch : 5 \t| Training Accuracy : 0.8759999871253967\n",
      "Epoch : 6 \t| Training Accuracy : 0.878000020980835\n",
      "Epoch : 7 \t| Training Accuracy : 0.8809999823570251\n",
      "Epoch : 8 \t| Training Accuracy : 0.8820000290870667\n",
      "Epoch : 9 \t| Training Accuracy : 0.8840000033378601\n",
      "Epoch : 10 \t| Training Accuracy : 0.8849999904632568\n",
      "Epoch : 11 \t| Training Accuracy : 0.906000018119812\n",
      "Epoch : 12 \t| Training Accuracy : 0.9570000171661377\n",
      "Epoch : 13 \t| Training Accuracy : 0.9639999866485596\n",
      "Epoch : 14 \t| Training Accuracy : 0.9660000205039978\n",
      "Epoch : 15 \t| Training Accuracy : 0.968999981880188\n",
      "Epoch : 16 \t| Training Accuracy : 0.972000002861023\n",
      "Epoch : 17 \t| Training Accuracy : 0.972000002861023\n",
      "Epoch : 18 \t| Training Accuracy : 0.972000002861023\n",
      "Epoch : 19 \t| Training Accuracy : 0.972000002861023\n",
      "Epoch : 20 \t| Training Accuracy : 0.9739999771118164\n",
      "Epoch : 21 \t| Training Accuracy : 0.9739999771118164\n",
      "Epoch : 22 \t| Training Accuracy : 0.9750000238418579\n",
      "Epoch : 23 \t| Training Accuracy : 0.9739999771118164\n",
      "Epoch : 24 \t| Training Accuracy : 0.9739999771118164\n",
      "Epoch : 25 \t| Training Accuracy : 0.9739999771118164\n",
      "Epoch : 26 \t| Training Accuracy : 0.9739999771118164\n",
      "Epoch : 27 \t| Training Accuracy : 0.9750000238418579\n",
      "Epoch : 28 \t| Training Accuracy : 0.9750000238418579\n",
      "Epoch : 29 \t| Training Accuracy : 0.9750000238418579\n",
      "Epoch : 30 \t| Training Accuracy : 0.9750000238418579\n",
      "Epoch : 31 \t| Training Accuracy : 0.9760000109672546\n",
      "Epoch : 32 \t| Training Accuracy : 0.9769999980926514\n",
      "Epoch : 33 \t| Training Accuracy : 0.9769999980926514\n",
      "Epoch : 34 \t| Training Accuracy : 0.9769999980926514\n",
      "Epoch : 35 \t| Training Accuracy : 0.9769999980926514\n",
      "Epoch : 36 \t| Training Accuracy : 0.9769999980926514\n",
      "Epoch : 37 \t| Training Accuracy : 0.9769999980926514\n",
      "Epoch : 38 \t| Training Accuracy : 0.9779999852180481\n",
      "Epoch : 39 \t| Training Accuracy : 0.9789999723434448\n",
      "Epoch : 40 \t| Training Accuracy : 0.9789999723434448\n",
      "Epoch : 41 \t| Training Accuracy : 0.9789999723434448\n",
      "Epoch : 42 \t| Training Accuracy : 0.9789999723434448\n",
      "Epoch : 43 \t| Training Accuracy : 0.9800000190734863\n",
      "Epoch : 44 \t| Training Accuracy : 0.9800000190734863\n",
      "Epoch : 45 \t| Training Accuracy : 0.9800000190734863\n",
      "Epoch : 46 \t| Training Accuracy : 0.9800000190734863\n",
      "Epoch : 47 \t| Training Accuracy : 0.9800000190734863\n",
      "Epoch : 48 \t| Training Accuracy : 0.9810000061988831\n",
      "Epoch : 49 \t| Training Accuracy : 0.9800000190734863\n",
      "Done training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch_number):\n",
    "    for i in range(iteration_number):\n",
    "        batch_x, batch_y = get_batch(size_of_batch, x_train, y_train)\n",
    "        feed_dictionary = {X:batch_x, Y:batch_y}\n",
    "        # running optimizer, running calcalation on given data\n",
    "        session.run(train_step, feed_dict=feed_dictionary)\n",
    "        \n",
    "        # using run method can give us an accuracy as output\n",
    "        # specify what the session should fetch, what output that we want to get >> fetchs\n",
    "        # fetch is when we asking our session to return some calculations\n",
    "        # accuracy is going to be calcalated on out batch\n",
    "    #batch_accuracy = session.run(fetches=[merged_summary, accuracy], feed_dict=feed_dictionary)  # like keras callbacks\n",
    "    s, batch_accuracy = session.run(fetches=[merged_summary, accuracy], feed_dict=feed_dictionary)\n",
    "    train_writer.add_summary(s, epoch)\n",
    "    print(f'Epoch : {epoch} \\t| Training Accuracy : {batch_accuracy.round(3)}')\n",
    "    \n",
    "    # validation\n",
    "    summary = session.run(fetches=merged_summary, feed_dict={X:x_validation, Y:y_validation})\n",
    "    validation_writer.add_summary(summary, epoch)\n",
    "    \n",
    "print('Done training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset for the Next Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_writer.close()\n",
    "validation_writer.close()\n",
    "session.close()\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
