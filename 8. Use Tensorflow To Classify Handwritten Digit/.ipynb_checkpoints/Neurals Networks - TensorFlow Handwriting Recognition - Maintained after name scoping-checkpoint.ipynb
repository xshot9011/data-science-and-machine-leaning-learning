{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\b\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\b\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\b\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\b\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\b\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\b\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\b\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\b\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\b\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\b\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\b\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\b\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(888)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(404)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from time import strftime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN_PATH = 'data/digit_xtrain.csv'\n",
    "X_TEST_PATH = 'data/digit_xtest.csv'\n",
    "Y_TRAIN_PATH = 'data/digit_ytrain.csv'\n",
    "Y_TEST_PATH = 'data/digit_ytest.csv'\n",
    "\n",
    "LOGGING_PATH = 'tensorboard_mnist_digit_logs/'\n",
    "\n",
    "NUMBER_CLASSES = 10\n",
    "VALIDATION_SIZE = 10000\n",
    "\n",
    "IMAGE_WIDTH = 28\n",
    "IMAGE_HEIGHT = 28\n",
    "CHANNELS = 1\n",
    "TOTAL_INPUTS = IMAGE_HEIGHT * IMAGE_WIDTH * CHANNELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "y_train_all = np.loadtxt(Y_TRAIN_PATH, delimiter=',', dtype=int)\n",
    "x_train_all = np.loadtxt(X_TRAIN_PATH, delimiter=',', dtype=int)\n",
    "x_test = np.loadtxt(X_TEST_PATH, delimiter=',', dtype=int)\n",
    "y_test = np.loadtxt(Y_TEST_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-scale to 0-1\n",
    "x_train_all, x_test = x_train_all/255.0, x_test/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conver target values to one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "(10000, 10)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "y_train_all = np.eye(NUMBER_CLASSES)[y_train_all]\n",
    "print(y_train_all.shape)\n",
    "y_test = np.eye(NUMBER_CLASSES)[y_test]\n",
    "print(y_test.shape)\n",
    "\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_validation = x_train_all[:VALIDATION_SIZE]\n",
    "y_validation = y_train_all[:VALIDATION_SIZE]\n",
    "x_train = x_train_all[VALIDATION_SIZE:]\n",
    "y_train = y_train_all[VALIDATION_SIZE:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Tensorflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rerun all below\n",
    "X = tf.placeholder(tf.float32, shape=[None, TOTAL_INPUTS], name='X')  \n",
    "# we leave the first dimension black, first dim is how many example we are going to use\n",
    "# this will actually be determined a little later on, when training model, we will split data to batch\n",
    "Y = tf.placeholder(tf.float32, shape=[None, NUMBER_CLASSES], name='labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_number = 50\n",
    "learning_rate = 0.001  # 1e-4 first time, 1e-3 second time\n",
    "\n",
    "hidden_1_number = 512\n",
    "hidden_2_number = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_layer(input_before, weight_dim, bias_dim, name):  # no histogram summary\n",
    "    with tf.name_scope(name):\n",
    "        initial_w = tf.truncated_normal(shape=weight_dim, stddev=0.1, seed=42)\n",
    "        w = tf.Variable(initial_value=initial_w, name='w')\n",
    "        \n",
    "        initial_bias = tf.constant(value=0.0, shape=bias_dim)\n",
    "        b = tf.Variable(initial_value=initial_bias, name='b')\n",
    "\n",
    "        layer_input = tf.matmul(input_before, w) + b\n",
    "        if name == 'out':\n",
    "            layer_output = tf.nn.softmax(layer_input)\n",
    "        else:\n",
    "            layer_output = tf.nn.relu(layer_input)\n",
    "        return layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_layer(input_before, weight_dim, bias_dim, name):  # with histogram summary\n",
    "    with tf.name_scope(name):\n",
    "        initial_w = tf.truncated_normal(shape=weight_dim, stddev=0.1, seed=42)\n",
    "        w = tf.Variable(initial_value=initial_w, name='w')\n",
    "        \n",
    "        initial_bias = tf.constant(value=0.0, shape=bias_dim)\n",
    "        b = tf.Variable(initial_value=initial_bias, name='b')\n",
    "\n",
    "        layer_input = tf.matmul(input_before, w) + b\n",
    "        if name == 'out':\n",
    "            layer_output = tf.nn.softmax(layer_input)\n",
    "        else:\n",
    "            layer_output = tf.nn.relu(layer_input)\n",
    "        \n",
    "        tf.summary.histogram('weights', w)\n",
    "        tf.summary.histogram('biases', b)\n",
    "        \n",
    "        return layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model without dropout\n",
    "#layer_1 = setup_layer(X, weight_dim=[TOTAL_INPUTS, hidden_1_number],\n",
    "#                     bias_dim=[hidden_1_number],\n",
    "#                     name='layer_1')\n",
    "#layer_2 = setup_layer(layer_1, weight_dim=[hidden_1_number, hidden_2_number],\n",
    "#                     bias_dim=[hidden_2_number],\n",
    "#                     name='layer_2')\n",
    "#output = setup_layer(layer_2, weight_dim=[hidden_2_number, NUMBER_CLASSES],\n",
    "#                     bias_dim=[NUMBER_CLASSES],\n",
    "#                     name='output')\n",
    "#model_name = f'{hidden_1_number}-{hidden_2_number} LR;{learning_rate} E;{epoch_number}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model dropout\n",
    "layer_1 = setup_layer(X, weight_dim=[TOTAL_INPUTS, hidden_1_number],\n",
    "                     bias_dim=[hidden_1_number],\n",
    "                     name='layer_1')\n",
    "layer_dropout = tf.nn.dropout(layer_1, rate=0.2, name='dropout_layer_1')\n",
    "layer_2 = setup_layer(layer_dropout, weight_dim=[hidden_1_number, hidden_2_number],\n",
    "                     bias_dim=[hidden_2_number],\n",
    "                     name='layer_2')\n",
    "output = setup_layer(layer_2, weight_dim=[hidden_2_number, NUMBER_CLASSES],\n",
    "                     bias_dim=[NUMBER_CLASSES],\n",
    "                     name='output')\n",
    "model_name = f'{hidden_1_number}-{hidden_2_number} LR;{learning_rate} E;{epoch_number}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully create file\n"
     ]
    }
   ],
   "source": [
    "\n",
    "folder_name = f'{model_name} at {strftime(\"%D %H%M\")}'.replace('/', ';')\n",
    "directory = os.path.join(LOGGING_PATH, folder_name)\n",
    "\n",
    "try:\n",
    "    os.makedirs(directory)\n",
    "except OSError as oserr:\n",
    "    print('error :', oserr.strerror)\n",
    "else:\n",
    "    print('Successfully create file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss, Optimisation and Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'): \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=output))\n",
    "with tf.name_scope('optimizer'):    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy_calc'):\n",
    "    correct_pred = tf.equal(tf.argmax(output, axis=1), tf.argmax(Y, axis=1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('performance'):\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('loss', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Input Images in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('show_image'):\n",
    "    x_image = tf.reshape(X, [-1, 28, 28, 1])  # reshape to picture shape\n",
    "    tf.summary.image('image_input', x_image, max_outputs=4)  # only 4 image willl show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "init = tf.global_variables_initializer()  # initialise all the variables\n",
    "session.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Filewriter and Merge Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_summary = tf.summary.merge_all()  # all the calcalation whta we want to read, we will combine and merge them and store it to this var\n",
    "train_writer = tf.summary.FileWriter(directory + '/train')\n",
    "train_writer.add_graph(session.graph)\n",
    "\n",
    "validation_writer = tf.summary.FileWriter(directory + '/validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batching the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_of_batch = 1000\n",
    "examples_number = y_train.shape[0]\n",
    "iteration_number = int(examples_number/size_of_batch)\n",
    "\n",
    "epoch_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(size_of_batch, data, labels):\n",
    "    global examples_number\n",
    "    global epoch_index\n",
    "    \n",
    "    start = epoch_index\n",
    "    epoch_index += size_of_batch\n",
    "    if epoch_index > examples_number:\n",
    "        start = 0\n",
    "        epoch_index = size_of_batch\n",
    "    end = epoch_index\n",
    "    \n",
    "    return data[start:end], labels[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 \t| Training Accuracy : 0.8700000047683716\n",
      "Epoch : 1 \t| Training Accuracy : 0.8809999823570251\n",
      "Epoch : 2 \t| Training Accuracy : 0.8859999775886536\n",
      "Epoch : 3 \t| Training Accuracy : 0.8870000243186951\n",
      "Epoch : 4 \t| Training Accuracy : 0.8930000066757202\n",
      "Epoch : 5 \t| Training Accuracy : 0.8980000019073486\n",
      "Epoch : 6 \t| Training Accuracy : 0.8970000147819519\n",
      "Epoch : 7 \t| Training Accuracy : 0.8999999761581421\n",
      "Epoch : 8 \t| Training Accuracy : 0.902999997138977\n",
      "Epoch : 9 \t| Training Accuracy : 0.902999997138977\n",
      "Epoch : 10 \t| Training Accuracy : 0.902999997138977\n",
      "Epoch : 11 \t| Training Accuracy : 0.902999997138977\n",
      "Epoch : 12 \t| Training Accuracy : 0.8999999761581421\n",
      "Epoch : 13 \t| Training Accuracy : 0.9049999713897705\n",
      "Epoch : 14 \t| Training Accuracy : 0.9049999713897705\n",
      "Epoch : 15 \t| Training Accuracy : 0.9039999842643738\n",
      "Epoch : 16 \t| Training Accuracy : 0.9049999713897705\n",
      "Epoch : 17 \t| Training Accuracy : 0.9070000052452087\n",
      "Epoch : 18 \t| Training Accuracy : 0.9079999923706055\n",
      "Epoch : 19 \t| Training Accuracy : 0.9039999842643738\n",
      "Epoch : 20 \t| Training Accuracy : 0.906000018119812\n",
      "Epoch : 21 \t| Training Accuracy : 0.9079999923706055\n",
      "Epoch : 22 \t| Training Accuracy : 0.9079999923706055\n",
      "Epoch : 23 \t| Training Accuracy : 0.9070000052452087\n",
      "Epoch : 24 \t| Training Accuracy : 0.906000018119812\n",
      "Epoch : 25 \t| Training Accuracy : 0.906000018119812\n",
      "Epoch : 26 \t| Training Accuracy : 0.906000018119812\n",
      "Epoch : 27 \t| Training Accuracy : 0.9079999923706055\n",
      "Epoch : 28 \t| Training Accuracy : 0.9079999923706055\n",
      "Epoch : 29 \t| Training Accuracy : 0.9079999923706055\n",
      "Epoch : 30 \t| Training Accuracy : 0.9079999923706055\n",
      "Epoch : 31 \t| Training Accuracy : 0.9079999923706055\n",
      "Epoch : 32 \t| Training Accuracy : 0.906000018119812\n",
      "Epoch : 33 \t| Training Accuracy : 0.9070000052452087\n",
      "Epoch : 34 \t| Training Accuracy : 0.9070000052452087\n",
      "Epoch : 35 \t| Training Accuracy : 0.9079999923706055\n",
      "Epoch : 36 \t| Training Accuracy : 0.9079999923706055\n",
      "Epoch : 37 \t| Training Accuracy : 0.9079999923706055\n",
      "Epoch : 38 \t| Training Accuracy : 0.9070000052452087\n",
      "Epoch : 39 \t| Training Accuracy : 0.9070000052452087\n",
      "Epoch : 40 \t| Training Accuracy : 0.9079999923706055\n",
      "Epoch : 41 \t| Training Accuracy : 0.9079999923706055\n",
      "Epoch : 42 \t| Training Accuracy : 0.9079999923706055\n",
      "Epoch : 43 \t| Training Accuracy : 0.9079999923706055\n",
      "Epoch : 44 \t| Training Accuracy : 0.9079999923706055\n",
      "Epoch : 45 \t| Training Accuracy : 0.9079999923706055\n",
      "Epoch : 46 \t| Training Accuracy : 0.9070000052452087\n",
      "Epoch : 47 \t| Training Accuracy : 0.9070000052452087\n",
      "Epoch : 48 \t| Training Accuracy : 0.9079999923706055\n",
      "Epoch : 49 \t| Training Accuracy : 0.9079999923706055\n",
      "Done training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch_number):\n",
    "    for i in range(iteration_number):\n",
    "        batch_x, batch_y = get_batch(size_of_batch, x_train, y_train)\n",
    "        feed_dictionary = {X:batch_x, Y:batch_y}\n",
    "        # running optimizer, running calcalation on given data\n",
    "        session.run(train_step, feed_dict=feed_dictionary)\n",
    "\n",
    "        s, batch_accuracy = session.run(fetches=[merged_summary, accuracy], feed_dict=feed_dictionary)\n",
    "    train_writer.add_summary(s, epoch)\n",
    "    print(f'Epoch : {epoch} \\t| Training Accuracy : {batch_accuracy.round(3)}')\n",
    "    \n",
    "    # validation\n",
    "    summary = session.run(fetches=merged_summary, feed_dict={X:x_validation, Y:y_validation})\n",
    "    validation_writer.add_summary(summary, epoch)\n",
    "print('Done training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reset for the Next Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_writer.close()\n",
    "validation_writer.close()\n",
    "session.close()\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
